#!/usr/bin/python

__author__ = 'vilelag'

import sys
import argparse
import numpy as np


def create_parsers():
    #parser for the main program
    parser = argparse.ArgumentParser(description='Generates a network starting model from vocab, wn file data and'
                                                 'its vector representation')
    parser.add_argument('-v', '-vocab', metavar='<file>', required=True,
                        help='Vocabulary file generated by word2vec after analysing a corpus *must be original corpus*')
    parser.add_argument('-b', '-bin', metavar='<file>', required=True,
                        help='Dictionary generated by word2vec using a reshaped corpus')
    parser.add_argument('-wn', metavar='<file>', required=True,
                        help='pseud-classes tree generated by word_net_tree.py')
    parser.add_argument('-o', '-output', metavar='<file>', default='generated_net.bin',
                        help='File where the generated net will be saved')
    return parser


def read_vocab(fin):
    with open(fin, 'r') as f:
        content = f.read().splitlines()
    words = dict()
    for line in content:
        tmp = line.split(' ')
        words[tmp[0]] = int(tmp[1])
    try:
        words['</s>'] = sys.maxint
    except KeyError:
        pass

    words = sorted(words, key=words.__getitem__, reverse=True)
    return words


def read_tree(fin):
    with open(fin, 'r') as f:
        content = f.read().splitlines()
    alias = dict()
    classes = list()
    for line in content:
        tmp = line.split(' ')
        ps_class = tmp[0]
        classes.append(ps_class)
        for word in tmp[1:]:
            alias[word] = ps_class

    return classes, alias


def word_representations(vocab, class_rep, word_inc, size):
    wr = dict()
    for word in vocab:
        # If we have any information about the word class inclusion
        if word in word_inc:
            wr[word] = 0.1*np.random.random(size) - 0.05
            wr[word] /= float(size)
            wr[word] += class_rep[word_inc[word]]
            wr[word] /= np.linalg.norm(wr[word])

        # If we don have any information about its inclusion, we do in the same fashion as word2vec
        else:
            wr[word] = np.random.random(size) - 0.5
            wr[word] /= float(size)

    return wr


def class_representations(path, used_words):
    with open(path) as f:
        content = f.read().splitlines()
    data = dict()
    words, size = content[0].split(' ')
    words = int(words)
    size = int(size)

    for i in range(1, words+1):
        temp = content[i].split(' ')
        if temp[0].lower() not in used_words:
            continue
        data[temp[0].lower()] = np.asarray([np.float64(x) for x in temp[1:-1]], dtype=np.float64)
        # Normalizing
        data[temp[0].lower()] *= 1 / np.linalg.norm(data[temp[0].lower()])

    return data, size


def save_net(wr, fout, vocab, size):
    with open(fout, 'w') as f:
        f.write('{} {}\n'.format(len(vocab), size))
        for word in vocab:
            # print type(wr[word])
            # print np.shape(wr[word])
            num = ' '.join(['{0:.6f}'.format(wr[word][i]) for i in xrange(size)])
            f.write('{} {} \n'.format(word, num))


def main():
    parser = create_parsers()
    args = vars(parser.parse_args())
    vocab = args['v']
    tree = args['wn']
    _out = args['o']
    bin = args['b']

    vocab = read_vocab(vocab)
    classes, wi = read_tree(tree)

    print len(classes)

    class_rep, size = class_representations(bin, classes)

    words_rep = word_representations(vocab, class_rep, wi, size)

    save_net(words_rep, _out, vocab, size)


main()