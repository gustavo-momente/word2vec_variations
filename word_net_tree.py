#!/usr/bin/python

__author__ = 'vilelag'

import sys
import argparse
from nltk.corpus import wordnet as wn
from random import random

def create_parsers():
    #parser for the main program
    parser = argparse.ArgumentParser(description='Generate word tree from a vocabulary using WordNet implementation '
                                                 'from NLTK.')
    parser.add_argument('-v', '-vocab', metavar='<file>', required=True,
                        help='Vocabulary file generated by word2vec after analysing a corpus')
    parser.add_argument('-o', '-output', metavar='<file>', default='wn_tree.txt',
                        help='File where the resulting tree will be saved')
    return parser


def read_vocab(fin):
    with open(fin, 'r') as f:
        content = f.read().splitlines()
    words = dict()
    for line in content:
        tmp = line.split(' ')
        words[tmp[0].lower()] = int(tmp[1])
    try:
        words['</s>'] = sys.maxint
    except KeyError:
        pass

    words = sorted(words, key=words.__getitem__, reverse=True)
    return words


def get_synonym(words, threshold):
    count = 0
    skip = 0
    _pass = 0
    _hash = dict()
    freq = float(threshold)/len(words)
    used_words = []

    while len(_hash) < threshold:
        for word in words:
            if word in used_words:
                skip += 1
                continue
            # progress reporting
            count += 1
            if count % 100 == 0:
                sys.stdout.write("\r{} {} {} {:3.3f}".format(_pass, len(used_words), skip, float(100*len(_hash))/(float(threshold))))
                sys.stdout.flush()

            if random() > freq or len(wn.synsets(word)) < 1:
                continue

            for syn in wn.synsets(word):
                for name in syn.lemma_names:
                    try:
                        if name not in _hash[word] and name in words and name not in used_words:
                            _hash[word].append(name)
                            used_words.append(name)
                    except KeyError:
                        if name in words and name not in used_words:
                            _hash[word] = [name]
                            used_words.append(name)

            if len(_hash) >= threshold:
                break
        _pass += 1
    print ''

    # now we try to cover most of possible vocabulary
    change = 1
    _max = len(words)
    count = 0
    skip = 0
    _pass = 0

    checked_words = []

    while len(used_words) != _max and change > 0:
        start = len(used_words)
        for ps_class in _hash:
            for word in _hash[ps_class]:

                count += 1
                if count % 100 == 0:
                    sys.stdout.write("\r{} {} {} {} {:3.3f}".format(_pass, change, len(used_words),
                                                                    skip, float(100*len(used_words))/_max))
                    sys.stdout.flush()

                if word in checked_words:
                    skip += 1
                    continue

                for syn in wn.synsets(word):
                    for name in syn.lemma_names:
                        if name not in _hash[ps_class] and name in words and name not in used_words and name not in checked_words:
                            _hash[ps_class].append(name)
                            used_words.append(name)
                        checked_words.append(name)

        _pass += 1
        change = len(used_words) - start
    print ''

    return _hash


def get_tree_by_closures(words, threshold):

    hypo = lambda s: s.hyponyms()

    count = 0
    skip = 0
    _pass = 0
    _hash = dict()
    freq = float(threshold)/len(words)
    used_words = []
    change = 1

    try:
        while len(_hash) < threshold and change > 0:
            start = len(_hash)
            for word in words:
                # progress reporting
                count += 1
                if count % 100 == 0:
                    pos = 1
                    sys.stdout.write("\r{} {:20} {} {} {} {:3.3f}".format(pos, word, _pass, len(used_words), skip, float(100*len(_hash))/(float(threshold))))
                    sys.stdout.flush()

                if word in used_words:
                    skip += 1
                    continue

                if random() > freq or len(wn.synsets(word)) < 1:
                    continue
                
                for syn in wn.synsets(word):
                    count += 1
                    if count % 100 == 0:
                        pos = 2
                        sys.stdout.write("\r{} {:20} {} {} {} {:3.3f}".format(pos, word, _pass, len(used_words), skip, float(100*len(_hash))/(float(threshold))))
                        sys.stdout.flush()

                    # first we put all the words in the same level
                    for name in syn.lemma_names:
                        count += 1
                        if count % 100 == 0:
                            pos = 3
                            sys.stdout.write("\r{} {:20} {} {} {} {:3.3f}".format(pos, word, _pass, len(used_words), skip, float(100*len(_hash))/(float(threshold))))
                            sys.stdout.flush()
                        try:
                            if name not in _hash[word] and name in words and name not in used_words:
                                _hash[word].append(name)
                                used_words.append(name)
                        except KeyError:
                            if name in words and name not in used_words:
                                _hash[word] = [name]
                                used_words.append(name)

                    # Then, we get the next level in the wordnet tree

                    for nl_syn in syn.closure(hypo):
                        for name in nl_syn.lemma_names:

                            count += 1
                            if count % 100 == 0:
                                pos = 4
                                sys.stdout.write("\r{} {:20} {} {} {} {:3.3f}".format(pos, word, _pass, len(used_words), skip, float(100*len(_hash))/(float(threshold))))
                                sys.stdout.flush()

                            try:
                                if name not in _hash[word] and name in words and name not in used_words:
                                    _hash[word].append(name)
                                    used_words.append(name)
                            except KeyError:
                                if name in words and name not in used_words:
                                    _hash[word] = [name]
                                    used_words.append(name)

                if len(_hash) >= threshold:
                    break
            _pass += 1
            change = len(_hash) - start
        sys.stdout.write("\r{} {} {} {:3.3f}".format(_pass, len(used_words), skip, float(100*len(_hash))/(float(threshold))))
        sys.stdout.flush()
    except KeyboardInterrupt:
        pass

    print '\nend'
    return _hash


def get_tree(words, threshold):
    hypo = lambda s: s.hyponyms()
    count = 0
    skip = 0
    _pass = 0
    _hash = dict()
    total = len(words)
    freq = float(threshold)/total
    used_words = []
    change = 1

    used_synsets_map = dict()

    while len(_hash) < threshold and change > 0:
        start = len(_hash)
        for word in words:

            # Progress reporting
            count += 1
            if count % 100 == 0:
                sys.stdout.write("\r{} {} {:3.3f}".format(_pass, len(used_words), float(100*len(_hash))/(float(threshold))))
                sys.stdout.flush()
            # If the word was referenced before, we don't continue
            if word in used_words:
                continue

            # Now we test if any of the synsets of the word was already referenced
            flag = 0
            for syn in wn.synsets(word):
                if syn.name in used_synsets_map:
                    # if it was, we add the word to the pseudo-class, pointed by the synset
                    flag = 1
                    _hash[used_synsets_map[syn.name]].append(word)
                    used_words.append(word)
                    break
            # If it was referenced we go to next word
            if flag == 1:
                continue

            # If it wasn't, we proceed by verifying if we'll take this word as a seed
            if random() > freq or len(wn.synsets(word)) < 1:
                continue


            # We proceed to put it and all its synonyms in the _hash dict
            # So first, we add it
            _hash[word] = [word]
            used_words.append(word)

            # And its synonyms
            for syn in wn.synsets(word):
                # We mark the synset as used, and to which word it points

                used_synsets_map[syn.name] = word

                for name in syn.lemma_names:
                    _name = name.lower()
                    # so for each synonym we check if it's in the vocabulary and if it wasn't used before
                    if _name not in _hash[word] and _name in words and _name not in used_words:
                        used_words.append(_name)
                        _hash[word].append(_name)

            #If we reached the number of seed we break
            if len(_hash) >= threshold:
                sys.stdout.write("\r{} {} {:3.3f}".format(_pass, len(used_words), float(100*len(_hash))/(float(threshold))))
                sys.stdout.flush()
                break
        # End of a pass we increase and report it
        change = len(_hash) - start
        _pass += 1
        sys.stdout.write("\r{} {} {:3.3f}".format(_pass, len(used_words), float(100*len(_hash))/(float(threshold))))
        sys.stdout.flush()
    print ''

    # Now we pass to the next level in the wordnet tree
    change = 1
    _pass = 0
    checked_words = []
    checked_synsets = ['restrain.v.01']
    _synsets = []
    try:
        while len(used_words) < total and change > 0:
            start = len(used_words)

            # for each pseudo-class we get its words, then for each word we get its synsets and for each of those
            # we look for their hyponyms, and then we add then to the pseudo-class.
            for ps_class in sorted(_hash, key=lambda x: len(_hash[x]), reverse=True):
                sys.stdout.write("\r{} {} {:3.3f} {:3.3f} {:20}".format(_pass, len(used_words),
                                                                                float(100*len(checked_words))/len(used_words),
                                                                                float(100*len(used_words))/total, ps_class))
                sys.stdout.flush()
                for word in _hash[ps_class]:
                    count += 1
                    if count % 50 == 1:
                        sys.stdout.write("\r{} {} {:3.3f} {:3.3f} {:20}".format(_pass, len(used_words),
                                                                                float(100*len(checked_words))/len(used_words),
                                                                                float(100*len(used_words))/total, ps_class))
                        sys.stdout.flush()
                    # if we already checked the hyponyms of that word, we continue to the next one
                    if word in checked_words:
                        continue

                    # So we get its synset
                    for syn in wn.synsets(word):
                        count += 1
                        if count % 50 == 1:
                            sys.stdout.write("\r{} {} {:3.3f} {:3.3f} {:20}".format(_pass, len(used_words),
                                                                                float(100*len(checked_words))/len(used_words),
                                                                                float(100*len(used_words))/total, ps_class))
                            sys.stdout.flush()
                        if syn.name in checked_synsets:
                            continue

                        #and go to deeper
                        for nl_syn in syn.closure(hypo):

                            count += 1
                            if count % 50 == 1:
                                sys.stdout.write("\r{} {} {:3.3f} {:3.3f} {:20}".format(_pass, len(used_words),
                                                                                float(100*len(checked_words))/len(used_words),
                                                                                float(100*len(used_words))/total, ps_class))
                                sys.stdout.flush()
                            if nl_syn.name in _synsets:
                                continue

                            for name in nl_syn.lemma_names:
                                _name = name.lower()
                                count += 1
                                if count % 50 == 1:
                                    sys.stdout.write("\r{} {} {:3.3f} {:3.3f} {:20}".format(_pass, len(used_words),
                                                                                    float(100*len(checked_words))/len(used_words),
                                                                                    float(100*len(used_words))/total, ps_class))
                                    sys.stdout.flush()

                                if _name in words and _name not in used_words:
                                    _hash[ps_class].append(_name)
                                    used_words.append(_name)

                            _synsets.append(nl_syn.name)
                        checked_synsets.append(syn.name)
                    #we mark the word as checked, but not its leaves
                    checked_words.append(word)
                if len(used_words) >= total:
                    break

            change = len(used_words) - start
            sys.stdout.write("\r{} {} {:3.3f} {:3.3f} {:20}".format(_pass, len(used_words),
                                                                                        float(100*len(checked_words))/len(used_words),
                                                                                        float(100*len(used_words))/total, ps_class))
            sys.stdout.flush()
            _pass += 1
    except KeyboardInterrupt:
        print ''
        print ps_class, word, syn, nl_syn, name
        sys.stdout.write("\r{} {} {:3.3f} {:3.3f} {:20} {:60}".format(_pass, len(used_words),
                                                                                    float(100*len(checked_words))/len(used_words),
                                                                                    float(100*len(used_words))/total, ps_class, name))
        sys.stdout.flush()
        pass

    print ''
    return _hash


def save_tree(fout, _hash):
    with open(fout, 'w') as f:
        for _class in sorted(_hash, reverse=True, key=lambda x: len(_hash[x])):
            f.write('{} {}\n'.format(_class, ' '.join(_hash[_class])))


def main():
    parser = create_parsers()
    args = vars(parser.parse_args())
    vocab = args['v']
    fout = args['o']
    threshold = 10000

    words = read_vocab(vocab)

    count = 0
    words_in_wordnet = []
    for word in words:
        if len(wn.synsets(word)) < 1:
            count += 1
        else:
            words_in_wordnet.append(word)

    print 'words in vocab: {}\nwords in wordnet: {}' \
          '\nwords not in wordnet: {}\nratio: {:3.3f}%'.format(len(words), len(words_in_wordnet), count,
                                                               float(100*len(words_in_wordnet))/len(words))

    # _hash = get_synonym(words_in_wordnet, threshold)
    _hash = get_tree(words_in_wordnet, threshold)
    save_tree(fout, _hash)


main()