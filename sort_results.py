#!/usr/bin/python
__author__ = 'vilelag'

import os
import argparse
import math
from multiprocessing import Pool


def create_parsers():
    #parser for the main program
    parser = argparse.ArgumentParser(description='Sort logs generated by compute-accuracy or SE-Test1.py, also'
                                                 ' works with average logs of those 2 tasks. For usage examples, check'
                                                 ' ./Tests/var_*/run_all.sh')
    parser.add_argument('-se', metavar='<int>', nargs='?', default=0, const=1, type=int,
                        help='Must be present to use logs generated using "SE-Test1.py" (1 : On, else: Off,'
                             ' Default: Off)')
    parser.add_argument('-f', '-folder', metavar='<folder>', default='./Logs/Log0',
                        help='Folder with logs generated by "run_dics.py"')
    parser.add_argument('-gdl', metavar='<file>', default='./Logs/out.csv',
                        help='csv generated by "generate_dics.py"')
    parser.add_argument('-o', '-out', metavar='<file.csv>', default='./output_sorted.csv',
                        help="csv file with the sorted output")
    parser.add_argument('-t', '-threads', metavar='<int>', nargs=1, default=[8], type=int,
                        help='Use <int> threads (default 8)')
    parser.add_argument('-cm', '-ca_mean', metavar='<int>', nargs='?', default=0, const=1, type=int,
                        help='Must be present to use logs genereated from averaging ca logs (1 : On, else: Off,'
                             ' Default: Off)')
    return parser


def read_log(log):
    with open(log) as f:
        content = f.read().splitlines()
    file_name = content[0]
    data = dict()
    for i in range(1, len(content)-1, 3):
        kind = content[i][:-1]
        top1 = float(content[i+1].split(" ")[2])

        line3 = content[i+2].split(" ")
        ta = float(line3[2])
        sem = float(line3[8])
        syn = float(line3[14])
        data[kind] = [top1, ta, sem, syn]

    ll = content[-1].split(" ")
    dic2 = {'seen': ll[4], 'total': ll[5]}
    return data, dic2


def read_log_ca_mean(log):
    with open(log) as f:
        content = f.read().splitlines()
    data = dict()
    for i in range(1, len(content), 1):
        _split = content[i].split(',')
        kind = _split[0]
        _data = [float(n) for n in _split[1:]]
        data[kind] = _data

    return data


def read_csv(csv):
    with open(csv) as f:
        content = f.read().splitlines()
    csv_data = dict()
    for i in range(1, len(content)):
        tmp = content[i].split(';')
        name = os.path.splitext(os.path.basename(tmp[0]))[0]
        size = int(tmp[4])
        window = int(tmp[5])
        sample = float(tmp[6])
        hs = int(tmp[7])
        negative = int(tmp[8])
        min_count = int(tmp[10])
        alpha = float(tmp[11])
        cbow = int(tmp[15])
        csv_data[name] = [size, window, sample, hs, negative, min_count, alpha, cbow]

    return csv_data


def read_se_log(log):
    with open(log) as f:
        content = f.read().splitlines()
    data = dict()
    for case in content[1:]:
        tmp = case.split(',')
        data[tmp[0]] = [float(tmp[1])]
    return data


def get_score(a_case):
    score = 0
    for key, component in a_case.iteritems():
        score += component[0]
    score /= len(a_case)
    return score


def get_se_score(a_case):
    return a_case['Average'][0]


def main():
    parser = create_parsers()
    args = vars(parser.parse_args())
    folder = args['f']
    gdl = args['gdl']
    threads = args['t'][0]
    out = args['o']
    se = args['se']
    cm = args['cm']
    csv_data = read_csv(gdl)
    log_data = dict()
    log_list = csv_data.keys()
    for log in log_list:
        try:
            if se == 0:
                if cm == 0:
                    log_data[log], trash = read_log(folder+'/'+log+'.log')
                else:
                    log_data[log] = read_log_ca_mean(folder+'/'+log+'.log')
            else:
                n = int(log.split('_')[-1].rstrip('.bin'))
                log_data[log] = read_se_log(folder+'/SE-2012/{}/log.csv'.format(n))
        except:
            print "Couldn't find {0}.log\n".format(log)
            log_data.pop(log, None)
            csv_data.pop(log, None)

    #get all scores in a parallel fashion
    log_parallel = [log_data[k] for k in sorted(log_data)]
    pool = Pool(threads)
    if se == 0:
        scores = pool.map(get_score, log_parallel)
    else:
        scores = pool.map(get_se_score, log_parallel)

    #associate a log to each score
    cont = 0
    score_d = dict()
    for k in sorted(log_data):
        score_d[k] = scores[cont]
        cont += 1

    best = max(scores)
    # fraction
    fract = dict()
    for k in log_data:
        fract[k] = float(score_d[k])/best


    #write output
    f = open(out, 'w')
    f.write(','.join(names)+'\n')

    for k in sorted(score_d, key=lambda x: score_d[x], reverse=True):
        f.write('{0:3.4f},'.format(score_d[k])+'{0:3.4f},'.format(fract[k])+k+','+','.join([str(i) for i in csv_data[k]])+'\n')
    f.close()

names = ['score', "score/best", 'file', 'size', 'window', 'sample', 'hs', 'negative', 'min-count', 'alpha', 'cbow']
main()
